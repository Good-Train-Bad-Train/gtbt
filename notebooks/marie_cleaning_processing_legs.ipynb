{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../goodtrainbadtrain/data/select2_2020.csv',sep=',')\n",
    "df2 = pd.read_csv('../goodtrainbadtrain/data/select2_2021.csv',sep=',')\n",
    "df3 = pd.read_csv('../goodtrainbadtrain/data/select2_2022.csv',sep=',')\n",
    "\n",
    "df = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for München and Köln\n",
    "df = df.query(\"bhf in ('München Hbf', 'Köln Hbf',  'Köln Messe/Deutz Gl.11-12',  'Frankfurt(M) Flughafen Fernbf', 'Mannheim Hbf',  'Stuttgart Hbf','Würzburg Hbf', 'Frankfurt(Main) Hbf','Nürnberg Hbf','Berlin Hbf', 'Berlin Hbf (tief)','Hamburg Hbf','Essen Hbf','Hagen(Westf) Bahnhof','Hagen Hbf','Hannover Hbf','Erfurt Hbf','Göttingen')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS AND CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) cleaning of train names \n",
    "#some letters of 'zugnr' are not capitalized\n",
    "df['zugnr'] = df['zugnr'].str.upper()\n",
    "\n",
    "#2)process of 9999 in arrTime and depTime: add new column with info \n",
    "df['start_or_endpoint'] = 'nan'\n",
    "df.loc[df['arrTime'] == 9999, 'start_or_endpoint'] = 'start'\n",
    "df.loc[df['depTime'] == 9999, 'start_or_endpoint'] = 'end'\n",
    "#overwrite 9999 with respective arr/dep time of same observation (in new clean columns)\n",
    "df['arrTime_clean'] = np.where(df['arrTime'] == 9999, df['depTime'], df['arrTime'])\n",
    "df['depTime_clean'] = np.where(df['depTime'] == 9999, df['arrTime'], df['depTime'])\n",
    "\n",
    "#3)some times need to be filled up with 0's. eg. '5' -> 00:05\n",
    "df['arrTime_clean'] = df['arrTime_clean'].astype(str)\n",
    "df['arrTime_clean'] = df['arrTime_clean'].map(lambda a: a.zfill(4))\n",
    "df['depTime_clean'] = df['depTime_clean'].astype(str)\n",
    "df['depTime_clean'] = df['depTime_clean'].map(lambda a: a.zfill(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGENEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/92j280qj2j971syqfbthtx0r0000gn/T/ipykernel_78827/681295108.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['city'] = df['city'].str.replace(key, city_dictionary[key])\n"
     ]
    }
   ],
   "source": [
    "#1)add city feature (merged Köln Hbf/ Messe Deutz)\n",
    "df['city'] = df.bhf\n",
    "\n",
    "city_dictionary = {'Köln Messe/Deutz Gl.11-12':'Köln',\n",
    "              'Köln Hbf':'Köln',\n",
    "              'München Hbf':'München',\n",
    "              'Frankfurt(M) Flughafen Fernbf':'Frankfurt',\n",
    "              'Frankfurt(Main) Hbf':'Frankfurt',\n",
    "              'Nürnberg Hbf':'Nürnberg',\n",
    "              'Stuttgart Hbf':'Stuttgart',\n",
    "              'Würzburg Hbf':'Würzburg',\n",
    "              'Mannheim Hbf':'Mannheim',\n",
    "              'Essen Hbf':'Essen',\n",
    "              'Hamburg Hbf':'Hamburg',\n",
    "              'Berlin Hbf':'Berlin',\n",
    "              'Berlin (tief)':'Berlin',\n",
    "              'Hannover Hbf':'Hannover',\n",
    "              'Berlin Hbf (tief)':'Berlin',\n",
    "              'Hagen Hbf':'Hagen',\n",
    "              'Erfurt Hbf':'Erfurt', \n",
    "              'Göttingen':'Göttingen'} \n",
    "\n",
    "for key in city_dictionary.keys():\n",
    "    df['city'] = df['city'].str.replace(key, city_dictionary[key])\n",
    "     \n",
    "#code above does not work for berlin and frankfurt; workaround:\n",
    "df['city'] = df['city'].apply(lambda x: x.replace('Frankfurt(M) Flughafen Fernbf','Frankfurt'))\n",
    "df['city'] = df['city'].apply(lambda x: x.replace('Berlin (tief)','Berlin'))\n",
    "\n",
    "#2) add date column\n",
    "df['date'] = df['datum'] + ' ' + df['arrTime_clean']\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H%M')\n",
    "\n",
    "#3a) add weekday \n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "\n",
    "#3a) add binary weekend-week \n",
    "df['weekend'] = (df['weekday'] == 'Sunday') | (df['weekday'] == 'Saturday')\n",
    "\n",
    "#4) add month of the year \n",
    "df['month'] = df['date'].dt.month_name()\n",
    "\n",
    "#time of the day\n",
    "df['time_of_day'] = pd.cut(pd.to_datetime(df.date).dt.hour,\n",
    "       bins=[0, 6, 12, 18, 24],\n",
    "       labels=['night', 'morning', 'afternoon', 'evening'],\n",
    "       right=False,\n",
    "       include_lowest=True)\n",
    "\n",
    "#5 time of day in circular \n",
    "seconds_in_day = 24*60*60\n",
    "df['seconds'] = pd.to_datetime(df.date).dt.time.astype(str)\n",
    "df['seconds'] = df['seconds'].map(lambda a: sum(x * int(t) for x, t in zip([3600, 60, 1], a.split(\":\"))))\n",
    "df['sin_time'] = np.sin(2*np.pi*df.seconds/seconds_in_day)\n",
    "df['cos_time'] = np.cos(2*np.pi*df.seconds/seconds_in_day)\n",
    "df.drop('seconds', axis=1, inplace=True)\n",
    "\n",
    "#6 circular day of the year\n",
    "from datetime import date\n",
    "days_in_year = 366\n",
    "df['day_of_year'] = df.date.map(lambda x: x.strftime('%j')).astype(int)\n",
    "df['sin_day'] = np.sin(2*np.pi*df.day_of_year/days_in_year)\n",
    "df['cos_day'] = np.cos(2*np.pi*df.day_of_year/days_in_year)\n",
    "df.drop('day_of_year', axis=1, inplace=True)\n",
    "\n",
    "#todo\n",
    "#7) add public holiday\n",
    "holidays = pd.read_csv('../goodtrainbadtrain/data/holidays.csv',sep=',')\n",
    "holidays = holidays['date'].to_list()\n",
    "\n",
    "def check_holiday(date):\n",
    "    if date in holidays:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df['public_holiday'] =  df.datum.map(lambda x: check_holiday(x))\n",
    "\n",
    "#8) add covid lockdown\n",
    "def process_time(intime, start, end):\n",
    "    if start <= intime <= end:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "start1 = dt.fromisoformat('2020-03-15'); end1 = dt.fromisoformat('2020-06-23')\n",
    "start2 = dt.fromisoformat('2021-01-10'); end2 = dt.fromisoformat('2021-05-24')\n",
    "\n",
    "df['covid_lockdown1'] = df['date'].map(lambda x: process_time(x, start1, end1))\n",
    "df['covid_lockdown2'] = df['date'].map(lambda x: process_time(x, start2, end2))\n",
    "\n",
    "df['covid_lockdown'] = df['covid_lockdown1'] | df['covid_lockdown2']\n",
    "df.drop('covid_lockdown1',axis=1, inplace=True)\n",
    "df.drop('covid_lockdown2',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD DIRECTON INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/92j280qj2j971syqfbthtx0r0000gn/T/ipykernel_78827/4022885334.py:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  journeys_long['origin_city'] = journeys_long['origin_city'].str.replace(key, city_dictionary[key])\n",
      "/var/folders/0v/92j280qj2j971syqfbthtx0r0000gn/T/ipykernel_78827/4022885334.py:53: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  journeys_long['destination_city'] = journeys_long['destination_city'].str.replace(key, city_dictionary[key])\n"
     ]
    }
   ],
   "source": [
    "journeys = pd.read_csv('../goodtrainbadtrain/data/journeylist_withberlin.csv')\n",
    "\n",
    "#processing\n",
    "journeys = journeys[journeys.leg1_train.notna()] #delete duplicated trips that once go to köln hbf and once to deutz\n",
    "journeys = journeys[journeys.leg2_train.notna()] \n",
    "journeys = journeys[journeys.leg3_train.notna()] \n",
    "\n",
    "journeys = journeys.drop(['Unnamed: 0'],axis=1) \n",
    "journeys['key_ID'] = list(range(journeys.shape[0]))\n",
    "\n",
    "#rename columns (necessary for wide_to_long function)\n",
    "journeys.columns = [ 'date', 'weekday', 'month', 'journey_origin',\n",
    "       'journey_destination', 'journey_start', 'journey_end',\n",
    "       'journey_duration', 'journey_numberlegs', \n",
    "       'train_leg1', 'origin_leg1','destination_leg1', 'start_leg1', 'end_leg1', 'duration_leg1',\n",
    "       'train_leg2', 'origin_leg2', 'destination_leg2', 'start_leg2','end_leg2', 'duration_leg2', \n",
    "       'train_leg3', 'origin_leg3','destination_leg3', 'start_leg3', 'end_leg3', 'duration_leg3', \n",
    "       'key_ID']\n",
    "\n",
    "journeys_long = pd.wide_to_long(df = journeys,\n",
    "                                stubnames=['train', 'origin','destination', 'start', 'end', 'duration'],\n",
    "                                i=['key_ID'],\n",
    "                                j='leg',\n",
    "                                sep = '_',\n",
    "                                suffix='.+').reset_index()\n",
    "\n",
    "#delete empty legs\n",
    "journeys_long = journeys_long[journeys_long.train != '-1']\n",
    "journeys_long = journeys_long[journeys_long.origin != '-1']\n",
    "\n",
    "#reorder columns\n",
    "journeys_long = journeys_long[['key_ID', \n",
    " 'journey_origin', 'journey_destination','journey_start','journey_end','journey_duration', 'journey_numberlegs',\n",
    " 'leg', 'train', 'origin', 'destination', 'start', 'end','duration',\n",
    " 'date', 'month','weekday']]\n",
    "\n",
    "def date_transformation(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.split('+', expand=True)[[0]]\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "journeys_long = date_transformation(journeys_long, ['journey_start', 'journey_end']) #, 'start', 'end'])\n",
    "journeys_long['month'] = journeys_long['journey_start'].dt.month_name()\n",
    "journeys_long['weekday'] = journeys_long['journey_start'].dt.day_name()\n",
    "\n",
    "#add columns with city name\n",
    "journeys_long['origin_city'] = journeys_long.origin\n",
    "journeys_long['destination_city'] = journeys_long.destination\n",
    "\n",
    "for key in city_dictionary.keys():\n",
    "    journeys_long['origin_city'] = journeys_long['origin_city'].str.replace(key, city_dictionary[key])\n",
    "    journeys_long['destination_city'] = journeys_long['destination_city'].str.replace(key, city_dictionary[key])\n",
    "    \n",
    "#code above does not work for berlin and frankfurt; workaround:\n",
    "journeys_long['origin_city'] = journeys_long['origin_city'].apply(lambda x: x.replace('Frankfurt(M) Flughafen Fernbf','Frankfurt'))\n",
    "journeys_long['origin_city'] = journeys_long['origin_city'].apply(lambda x: x.replace('Frankfurt(M) Flughafen Fernbf','Frankfurt'))\n",
    "journeys_long['destination_city'] = journeys_long['destination_city'].apply(lambda x: x.replace('Berlin (tief)','Berlin'))\n",
    "journeys_long['destination_city'] = journeys_long['destination_city'].apply(lambda x: x.replace('Berlin (tief)','Berlin'))\n",
    "\n",
    "\n",
    "#add column trip (eg. Berlin-Köln)\n",
    "journeys_long['trip'] = journeys_long['origin_city'] + '-' + journeys_long['destination_city']\n",
    "\n",
    "ice_df = journeys_long[['origin_city','destination_city','trip','train']].groupby(['origin_city','destination_city','trip'])['train'].apply(','.join).reset_index()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for index, row in ice_df.iterrows():\n",
    "    data = df[df.city == row.destination_city]\n",
    "    data = data[data['zugnr'].isin(row.train.split(','))]\n",
    "    data['trip'] = row.trip\n",
    "    data['origin_city'] = row.origin_city\n",
    "    data['destination_city'] = row.destination_city\n",
    "    df_list.append(data) \n",
    "\n",
    "df = pd.concat(df_list, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) add weather variables\n",
    "df['sharp_date'] = df['date'].dt.round('H')\n",
    "#df['sharp_date']  = df.sharp_date.astype('str')\n",
    "\n",
    "cgn_data = pd.read_csv('../goodtrainbadtrain/data/koln.csv', parse_dates=['time'])\n",
    "muc_data = pd.read_csv('../goodtrainbadtrain/data/munchen.csv', parse_dates=['time'])\n",
    "fra_data = pd.read_csv('../goodtrainbadtrain/data/frankfurt.csv', parse_dates=['time'])\n",
    "man_data = pd.read_csv('../goodtrainbadtrain/data/mannheim.csv', parse_dates=['time'])\n",
    "nur_data = pd.read_csv('../goodtrainbadtrain/data/nurnberg.csv', parse_dates=['time'])\n",
    "stu_data = pd.read_csv('../goodtrainbadtrain/data/stuttgart.csv', parse_dates=['time'])\n",
    "wur_data = pd.read_csv('../goodtrainbadtrain/data/wurzburg.csv', parse_dates=['time'])\n",
    "ber_data = pd.read_csv('../goodtrainbadtrain/data/berlin.csv', parse_dates=['time'])\n",
    "erf_data = pd.read_csv('../goodtrainbadtrain/data/erfurt.csv', parse_dates=['time'])\n",
    "ess_data = pd.read_csv('../goodtrainbadtrain/data/essen.csv', parse_dates=['time'])\n",
    "got_data = pd.read_csv('../goodtrainbadtrain/data/gottingen.csv', parse_dates=['time'])\n",
    "hag_data = pd.read_csv('../goodtrainbadtrain/data/hagen.csv', parse_dates=['time'])\n",
    "ham_data = pd.read_csv('../goodtrainbadtrain/data/hamburg.csv', parse_dates=['time'])\n",
    "han_data = pd.read_csv('../goodtrainbadtrain/data/hannover.csv', parse_dates=['time'])\n",
    "\n",
    "\n",
    "#weather = {\"Köln\": cgn_data, \n",
    "#           \"München\": muc_data, \n",
    "#           \"Frankfurt\": fra_data, \n",
    "#           \"Mannheim\": man_data, \n",
    "#           \"Nürnberg\": nur_data, \n",
    "#           \"Stuttgart\": stu_data,\n",
    "#           \"Würzburg\": wur_data,\n",
    "#           \"Berlin\": ber_data,\n",
    "#           \"Erfurt\" :erf_data,\n",
    "#            \"Essen\": ess_data,\n",
    "#            \"Göttingen\":got_data,\n",
    "#            \"Hagen\":hag_data,\n",
    "#            \"Hamburg\":ham_data,\n",
    "#            \"Hannover\":han_data\n",
    "#           }\n",
    "\n",
    "#for k, w_df in weather.items():\n",
    "#    w_df['snow'] = w_df['snow'].replace(np.nan, 0)\n",
    "\n",
    "#df = total_df.drop(columns=['dwpt', 'rhum', 'wdir', 'pres', 'tsun'])\n",
    "#df['snow'] = df['snow'].replace(np.nan, 0)\n",
    "\n",
    "# Load coco file\n",
    "coco = pd.read_csv('../goodtrainbadtrain/data/weather_coco.csv', sep=';')\n",
    "coco.set_index('Code', inplace=True)\n",
    "coco = coco.to_dict()['Weather Condition']\n",
    "\n",
    "# Define new classification for coco\n",
    "new_classes = {\n",
    "    '1': [1, 2],\n",
    "    '2': [3, 4, 7, 14],\n",
    "    '3': [5, 8, 10, 12, 15, 17, 19, 21, 23, 24, 25],\n",
    "    '4': [6, 9, 11, 13, 16, 18, 20, 22, 26, 27]\n",
    "}\n",
    "\n",
    "# Apply new classification for coco\n",
    "reclass = {}\n",
    "for k, values in new_classes.items():\n",
    "    for v in values:\n",
    "        for c in range(1, 28):\n",
    "            if v == c:\n",
    "                reclass[v] = k\n",
    "\n",
    "reclass = dict(sorted(reclass.items()))\n",
    "\n",
    "cgn_data['coco'] = cgn_data['coco'].map(reclass).astype('int')\n",
    "muc_data['coco'] = muc_data['coco'].map(reclass).astype('int')\n",
    "ber_data['coco'] = ber_data['coco'].map(reclass).astype('int')\n",
    "\n",
    "cgn_data['snow'] = cgn_data['snow'].replace(np.nan, 0)\n",
    "muc_data['snow'] = muc_data['snow'].replace(np.nan, 0)\n",
    "ber_data['snow'] = ber_data['snow'].replace(np.nan, 0)\n",
    "\n",
    "cgn_data = cgn_data[['time','temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']]\n",
    "muc_data = muc_data[['time','temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']]\n",
    "ber_data = ber_data[['time','temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']]\n",
    "\n",
    "weather_merged = cgn_data.merge(muc_data, how='left', left_on='time', right_on='time')\n",
    "weather_merged = weather_merged.merge(ber_data, how='left', left_on='time', right_on='time')\n",
    "\n",
    "weather_merged['temp_max'] = weather_merged[['temp', 'temp_x','temp_y']].max(axis=1)\n",
    "weather_merged['temp_min'] = weather_merged[['temp', 'temp_x','temp_y']].min(axis=1)\n",
    "weather_merged['prcp_max'] = weather_merged[['prcp', 'prcp_x','prcp_y']].max(axis=1)\n",
    "weather_merged['snow_max'] = weather_merged[['snow', 'snow_x','snow_y']].max(axis=1)\n",
    "weather_merged['wspd_max'] = weather_merged[['wspd', 'wspd_x','wspd_y']].max(axis=1)\n",
    "weather_merged['wpgt_max'] = weather_merged[['wpgt', 'wpgt_x','wpgt_y']].max(axis=1)\n",
    "weather_merged['coco_max'] = weather_merged[['coco', 'coco_x','coco_y']].max(axis=1)\n",
    "\n",
    "weather_merged = weather_merged[['time','temp_max','temp_min', 'prcp_max', 'snow_max', 'wspd_max', 'wpgt_max', 'coco_max']]\n",
    "\n",
    "df['time_6_before'] = df.sharp_date - timedelta(hours=int(6))\n",
    "df['time_12_before'] = df.sharp_date - timedelta(hours=int(12))\n",
    "\n",
    "df = df.merge(weather_merged, how='left',left_on='sharp_date',right_on='time')\n",
    "df = df.merge(weather_merged, how='left',left_on='time_6_before',right_on='time')\n",
    "df = df.merge(weather_merged, how='left',left_on='time_12_before',right_on='time')\n",
    "\n",
    "df['temp_max_combined'] = df[['temp_max', 'temp_max_x','temp_max_y']].max(axis=1)\n",
    "df['temp_max_combined'] = df[['temp_max', 'temp_max_x','temp_max_y']].max(axis=1)\n",
    "df['temp_min_combined'] = df[['temp_min', 'temp_min_x','temp_min_y']].min(axis=1)\n",
    "df['prcp_max_combined'] = df[['prcp_max', 'prcp_max_x','prcp_max_y']].max(axis=1)\n",
    "df['snow_max_combined'] = df[['snow_max', 'snow_max_x','snow_max_y']].max(axis=1)\n",
    "df['wspd_max_combined'] = df[['wspd_max', 'wspd_max_x','wspd_max_y']].max(axis=1)\n",
    "df['wpgt_max_combined'] = df[['wpgt_max', 'wpgt_max_x','wpgt_max_y']].max(axis=1)\n",
    "df['coco_max_combined'] = df[['coco_max', 'coco_max_x','coco_max_y']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for keys, values in weather.items():\n",
    "#for keys, values in weather.items():\n",
    "#    weather_df = values[['time','temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']]\n",
    "#    test_df = test_df.merge(weather_df, how='left',left_on='sharp_date',right_on='time')\n",
    "\n",
    "\n",
    "# def weather_transform(row, cases, weather_df):\n",
    "#     for \n",
    "#     v, c, t = case.split('_')\n",
    "#     return weather_df.loc[weather_df['time'] == row['sharp_date'] - timedelta(hours=int(t)), v].reset_index(drop=True).loc[0].copy()\n",
    "    \n",
    "# c_hours = [6, 12]\n",
    "# c_cities = ['oc', 'dc'] # oc: origin city, dc: destination city\n",
    "# c_variables = ['temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']\n",
    "\n",
    "# cases = [v + '_' + c + '_' + str(h) for h in c_hours for c in c_cities for v in c_variables] #list of origin and time combination\n",
    "# for c in list(weather.keys()):\n",
    "#     test_df[cases] = test_df.apply(lambda row: weather_transform(row, cases, weather[c]), axis=1)\n",
    "\n",
    "#def weather_transform(row, case, weather_df):\n",
    "#    v, c, t = case.split('_')\n",
    "#    return weather_df.loc[weather_df['time'] == row['sharp_date'] - timedelta(hours=int(t)), v].reset_index(drop=True).loc[0].copy()\n",
    "    \n",
    "#c_hours = [6, 12]\n",
    "#c_cities = ['oc', 'dc'] # oc: origin city, dc: destination city\n",
    "#c_variables = ['temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']\n",
    "\n",
    "#cases = [v + '_' + c + '_' + str(h) for h in c_hours for c in c_cities for v in c_variables] #list of origin and time combination\n",
    "\n",
    "\n",
    "#def weather_transform(row, case, weather_df):\n",
    "#    v, c, t = case.split('_')\n",
    "#    return weather_df.loc[weather_df['time'] == row['sharp_date'] - timedelta(hours=int(t)), v].reset_index(drop=True).loc[0].copy()\n",
    "    \n",
    "#c_hours = [6, 12]\n",
    "#c_cities_oc = ['oc'] # oc: origin city, dc: destination city\n",
    "#c_cities_dc = ['dc']\n",
    "#c_variables = ['temp', 'prcp', 'snow', 'wspd', 'wpgt', 'coco']\n",
    "\n",
    "#cases_dc = [v + '_' + c + '_' + str(h) for h in c_hours for c in c_cities_dc for v in c_variables] \n",
    "#cases_oc = [v + '_' + c + '_' + str(h) for h in c_hours for c in c_cities_oc for v in c_variables] #list of origin and time combination\n",
    "\n",
    "#df_list = list()\n",
    "#for c in list(weather.keys())[5]:\n",
    "#    test_df = df[df.origin_city == c]\n",
    "#    for case in cases:\n",
    "#        test_df[case] = test_df.apply(lambda row: weather_transform(row, case, weather[c]), axis=1)\n",
    "#    df_list.append(test_df)\n",
    "\n",
    "#c= 'Stuttgart'\n",
    "#case= cases_oc[0] #'temp_oc_6'\n",
    "#test_df = df[df.origin_city == c]\n",
    "\n",
    "#for case in cases:\n",
    "#        test_df[case] = test_df.apply(lambda row: weather_transform(row, case, weather[c]), axis=1)\n",
    "#df_list.append(test_df)\n",
    "\n",
    "#total_df =  pd.DataFrame()\n",
    "#for station, w_df in weather.items():\n",
    "#    weather_data = weather[station]\n",
    "#    weather_data['sharp_date']  = weather_data.time.astype('str')\n",
    "#    station_df = pd.merge(df[df['city'] == station], weather_data, how='left', left_on='sharp_date', right_on='sharp_date')\n",
    "#    total_df = pd.concat([total_df, station_df])\n",
    "\n",
    "#total_df =  pd.DataFrame()\n",
    "#for station, w_df in weather.items():\n",
    "#    weather_data = weather[station]\n",
    "#    weather_data['sharp_date']  = weather_data.time.astype('str')\n",
    "#    station_df = pd.merge(df[df['city'] == station], weather_data, how='left', left_on='sharp_date', right_on='sharp_date')\n",
    "#    total_df = pd.concat([total_df, station_df])\n",
    "\n",
    "#total_df =  pd.DataFrame()\n",
    "#for trip in df.trip.unique():\n",
    "#    subset = df[df.trip == trip]\n",
    "#    origin = subset.origin_city[0]\n",
    "#    destination = subset.destination_city[0]\n",
    "#    weather_data_org = weather[origin]\n",
    "#    weather_data_dest = weather[destination]\n",
    "#    for index, row in subset.iterrows():\n",
    "#        time = row.sharp_date\n",
    "#        index_org = weather_data_org.index[weather_data_org.sharp_date == time]\n",
    "#        index_dest = weather_data_dest.index[weather_data_dest.sharp_date == time]\n",
    "#        org_6 = weather_data_org.loc[index_org-6]\n",
    "#        org_12 = weather_data_org.loc[index_org-12]\n",
    "#        dest_6 = weather_data_org.loc[index_org-6]\n",
    "#        dest_12 = weather_data_org.loc[index_org-12]\n",
    "#        org_6 = org_6.drop(columns=['dwpt', 'rhum', 'wdir', 'pres', 'tsun'])\n",
    "#        org_12 = org_12.drop(columns=['dwpt', 'rhum', 'wdir', 'pres', 'tsun'])\n",
    "#        dest_6 = dest_6.drop(columns=['dwpt', 'rhum', 'wdir', 'pres', 'tsun'])\n",
    "#        dest_12 = dest_12.drop(columns=['dwpt', 'rhum', 'wdir', 'pres', 'tsun'])\n",
    "#        org_6 = org_6.rename(columns=['temp_org_6', 'prcp_org_6', 'snow_org_6', 'wspd_org_6', 'wpgt_org_6', 'coco_org_6'])\n",
    "#        org_12 = org_12.rename(columns=['temp_org_12', 'prcp_org_12', 'snow_org_12', 'wspd_org_12', 'wpgt_org_12', 'coco_org_12'])\n",
    "#        dest_6 = dest_6.rename(columns=['temp_dest_6', 'prcp_dest_6', 'snow_dest_6', 'wspd_dest_6', 'wpgt_dest_6', 'coco_dest_6'])\n",
    "#        dest_12 = dest_12.rename(columns=['temp_dest_12', 'prcp_dest_12', 'snow_dest_12', 'wspd_dest_12', 'wpgt_dest_12','coco_dest_12'])\n",
    "#        new_row = pd.concat([row, org_6, org_12, dest_6, dest_12], axis=1)\n",
    "#        total_df = pd.concat([total_df, new_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TARGET PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a) into several categories\n",
    "# adelay-> into categories: no delay, small delay, medium delay, (big delay/cancellation)\n",
    "max = df.adelay.max()\n",
    "bins = [-2, -0.1, 0, 5, 30, max]\n",
    "group_names = ['large delay/cancelled','on time','small delay', 'medium delay', 'large delay/cancelled']\n",
    "df['target'] = pd.cut(df['adelay'], bins, labels=group_names, ordered=False)\n",
    "\n",
    "#1b) into two categories: good - bad train\n",
    "df['target_good_bad'] = df['target'].str.replace('large delay/cancelled','bad train')\n",
    "df['target_good_bad'] = df['target_good_bad'].str.replace('medium delay','bad train')\n",
    "\n",
    "df['target_good_bad'] = df['target_good_bad'].str.replace('on time','good train')\n",
    "df['target_good_bad'] = df['target_good_bad'].str.replace('small delay','good train')\n",
    "\n",
    "df['target_good_bad'] = df['target_good_bad'] == 'good train'\n",
    "\n",
    "#value counts of target:\n",
    "#on time                  67336\n",
    "#small delay              28649\n",
    "#medium delay             23617\n",
    "#large delay/cancelled    10677\n",
    "\n",
    "#2) binary target (on time - or not)\n",
    "df['target_binary'] =  (df['adelay'] == 0)*1\n",
    "\n",
    "#3) numeric target (cancelled and extreme values = 120 Min)\n",
    "df['target_numeric'] = df['adelay']\n",
    "df['target_numeric'] = np.where(df['target_numeric'] == -1, 120, df['target_numeric'])\n",
    "df['target_numeric'] = np.where(df['target_numeric'] > 120, 120, df['target_numeric'])\n",
    "#sns.boxplot(df['target_numeric'])\n",
    "\n",
    "##Add summarized delay of each train number\n",
    "df['target_numeric'] = df['adelay']\n",
    "df['target_numeric'] = np.where(df['target_numeric'] == -1, 120, df['target_numeric'])\n",
    "df['target_numeric'] = np.where(df['target_numeric'] > 120, 120, df['target_numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_delay_ices = df[['zugnr','target_numeric']].groupby('zugnr').mean('target_numeric').reset_index()\n",
    "mean_delay_ices.columns = ['zugnr', 'mean_delay']\n",
    "mean_delay_ices.to_csv('mean_delay_ices.csv')\n",
    "df = df.merge(mean_delay_ices, how='left', left_on='zugnr', right_on='zugnr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRELATION / TARGET VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.weekday, y=df.target_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.public_holiday, y=df.target_good_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.weekday, y=df.target_good_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.weekend, y=df.target_good_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.covid_lockdown, y=df.target_good_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.month, y=df.target_good_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary= df.target_good_bad == True\n",
    "#sns.barplot( x=binary, y= df.sin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary= df.target_good_bad == True\n",
    "#sns.barplot( x=binary, y= df.cos_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(x= df.sin_time, y=df.cos_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x= df.target_good_bad, y= df.temp_max_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['zugnr', 'datum', 'bhf', 'adelay','ddelay',\n",
    "'arrTime_clean', 'depTime_clean', 'city', 'date',\n",
    "'weekday', 'weekend', 'month', 'time_of_day', 'sin_time', 'cos_time',\n",
    "'sin_day', 'cos_day', 'public_holiday', 'covid_lockdown', 'trip',\n",
    "'origin_city', 'destination_city', 'sharp_date', \n",
    "'temp_max_combined', 'temp_min_combined',\n",
    "'prcp_max_combined', 'snow_max_combined', 'wspd_max_combined',\n",
    "'wpgt_max_combined', 'coco_max_combined', 'target',\n",
    "'target_good_bad', 'target_binary', 'target_numeric', 'mean_delay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../goodtrainbadtrain/data/data_for_model_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip</th>\n",
       "      <th>mean_delay</th>\n",
       "      <th>weekend</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>public_holiday</th>\n",
       "      <th>covid_lockdown</th>\n",
       "      <th>temp_max_combined</th>\n",
       "      <th>temp_min_combined</th>\n",
       "      <th>prcp_max_combined</th>\n",
       "      <th>snow_max_combined</th>\n",
       "      <th>wspd_max_combined</th>\n",
       "      <th>wpgt_max_combined</th>\n",
       "      <th>coco_max_combined</th>\n",
       "      <th>target_good_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berlin-Hannover</td>\n",
       "      <td>7.991597</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.165048</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>-0.051479</td>\n",
       "      <td>0.998674</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berlin-Hannover</td>\n",
       "      <td>7.991597</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.165048</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3.9</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berlin-Hannover</td>\n",
       "      <td>7.991597</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.065403</td>\n",
       "      <td>0.997859</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>0.996318</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berlin-Hannover</td>\n",
       "      <td>7.991597</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.165048</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>0.204552</td>\n",
       "      <td>0.978856</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berlin-Hannover</td>\n",
       "      <td>7.991597</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.165048</td>\n",
       "      <td>0.986286</td>\n",
       "      <td>0.320423</td>\n",
       "      <td>0.947274</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200037</th>\n",
       "      <td>Würzburg-Köln</td>\n",
       "      <td>9.825666</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.321439</td>\n",
       "      <td>-0.946930</td>\n",
       "      <td>0.565345</td>\n",
       "      <td>-0.824855</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>51.8</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200038</th>\n",
       "      <td>Würzburg-Köln</td>\n",
       "      <td>9.825666</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.321439</td>\n",
       "      <td>-0.946930</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>-0.834438</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200039</th>\n",
       "      <td>Würzburg-Köln</td>\n",
       "      <td>9.825666</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.321439</td>\n",
       "      <td>-0.946930</td>\n",
       "      <td>0.536696</td>\n",
       "      <td>-0.843776</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200040</th>\n",
       "      <td>Würzburg-Köln</td>\n",
       "      <td>9.825666</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.321439</td>\n",
       "      <td>-0.946930</td>\n",
       "      <td>0.522133</td>\n",
       "      <td>-0.852864</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200041</th>\n",
       "      <td>Würzburg-Köln</td>\n",
       "      <td>9.825666</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.321439</td>\n",
       "      <td>-0.946930</td>\n",
       "      <td>0.507415</td>\n",
       "      <td>-0.861702</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200042 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   trip  mean_delay  weekend  sin_time  cos_time   sin_day  \\\n",
       "0       Berlin-Hannover    7.991597     True -0.165048  0.986286 -0.051479   \n",
       "1       Berlin-Hannover    7.991597    False -0.165048  0.986286  0.017166   \n",
       "2       Berlin-Hannover    7.991597     True -0.065403  0.997859  0.085731   \n",
       "3       Berlin-Hannover    7.991597     True -0.165048  0.986286  0.204552   \n",
       "4       Berlin-Hannover    7.991597     True -0.165048  0.986286  0.320423   \n",
       "...                 ...         ...      ...       ...       ...       ...   \n",
       "200037    Würzburg-Köln    9.825666     True -0.321439 -0.946930  0.565345   \n",
       "200038    Würzburg-Köln    9.825666     True -0.321439 -0.946930  0.551102   \n",
       "200039    Würzburg-Köln    9.825666    False -0.321439 -0.946930  0.536696   \n",
       "200040    Würzburg-Köln    9.825666    False -0.321439 -0.946930  0.522133   \n",
       "200041    Würzburg-Köln    9.825666    False -0.321439 -0.946930  0.507415   \n",
       "\n",
       "         cos_day  public_holiday  covid_lockdown  temp_max_combined  \\\n",
       "0       0.998674           False           False                3.0   \n",
       "1       0.999853            True           False                3.9   \n",
       "2       0.996318           False           False                7.4   \n",
       "3       0.978856           False           False                7.7   \n",
       "4       0.947274           False           False                6.0   \n",
       "...          ...             ...             ...                ...   \n",
       "200037 -0.824855           False           False               17.3   \n",
       "200038 -0.834438           False           False               14.9   \n",
       "200039 -0.843776           False           False               16.0   \n",
       "200040 -0.852864           False           False               21.3   \n",
       "200041 -0.861702           False           False               21.8   \n",
       "\n",
       "        temp_min_combined  prcp_max_combined  snow_max_combined  \\\n",
       "0                    -3.5                0.0                0.0   \n",
       "1                    -2.1                0.0                0.0   \n",
       "2                    -0.8                0.0                0.0   \n",
       "3                    -0.2                0.0                0.0   \n",
       "4                    -0.5                1.1               20.0   \n",
       "...                   ...                ...                ...   \n",
       "200037                9.4                0.2                0.0   \n",
       "200038                7.1                0.4                0.0   \n",
       "200039                5.1                0.0                0.0   \n",
       "200040                9.0                0.0                0.0   \n",
       "200041               10.0                0.0                0.0   \n",
       "\n",
       "        wspd_max_combined  wpgt_max_combined  coco_max_combined  \\\n",
       "0                    15.5               24.0                  2   \n",
       "1                    20.9               30.0                  2   \n",
       "2                    13.0               22.2                  2   \n",
       "3                    22.2               42.6                  2   \n",
       "4                    10.8               27.0                  3   \n",
       "...                   ...                ...                ...   \n",
       "200037               24.1               51.8                  4   \n",
       "200038               18.5               41.0                  3   \n",
       "200039               16.7               33.3                  2   \n",
       "200040               14.8               32.0                  2   \n",
       "200041               18.4               35.0                  2   \n",
       "\n",
       "        target_good_bad  \n",
       "0                  True  \n",
       "1                  True  \n",
       "2                  True  \n",
       "3                  True  \n",
       "4                  True  \n",
       "...                 ...  \n",
       "200037             True  \n",
       "200038             True  \n",
       "200039             True  \n",
       "200040            False  \n",
       "200041             True  \n",
       "\n",
       "[200042 rows x 17 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my suggestion for features in the model:\n",
    "\n",
    "df[[#trip info eg. 'Köln-Berlin'\n",
    "    'trip',\n",
    "    #mean delay of that ICE as numeric feature \n",
    "    #(file that translates ICE to mean delay: 'mean_delay_ices.csv')\n",
    "    'mean_delay', \n",
    "\n",
    "    #translations of arrival time and date\n",
    "    'weekday', #categorical variable; we could also use 'weekend' as binary (1 means weekend day)\n",
    "    'sin_time', 'cos_time', #circular feature of time of the day\n",
    "    'sin_day', 'cos_day', #circular feature of day of the year\n",
    "\n",
    "    #special flagged days (both binary)\n",
    "    'public_holiday', 'covid_lockdown', \n",
    "\n",
    "    #extreme weather values from different time points\n",
    "    'temp_max_combined', 'temp_min_combined','prcp_max_combined', 'snow_max_combined', 'wspd_max_combined','wpgt_max_combined', 'coco_max_combined', \n",
    "\n",
    "    #target: 0=bad train, 1= good train\n",
    "    'target_good_bad']]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6399be11364a6cb6a0c6ff7c8d07942c7c341f551218ac7c514ba1a0e828f38a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('goodtrainbadtrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
